# ğŸ§± Mini Data Engineering Project â€” Superstore ETL Pipeline

This is a beginner-friendly **Data Engineering + Data Analysis** project built from scratch using Python.

---

## ğŸš€ Overview

This project demonstrates a simple **ETL (Extract â†’ Transform â†’ Load)** pipeline using the **Superstore Sales Dataset**.

**Goal:**  
To clean, aggregate, and analyze retail data to identify top-performing regions, categories, and profit margins.

---

## ğŸ§© Steps

### 1. Extract
- Ingested raw Superstore CSV data.
- Handled encoding and missing values.
- Stored staged data in `/data/staged/`.

### 2. Transform
- Cleaned column names and data types.
- Calculated total sales, profit, and profit margins.
- Saved to `/data/clean/`.

### 3. Load
- Aggregated metrics by region and category.
- Exported a final dataset to `/data/warehouse/final_superstore.csv`.

### 4. Explore
- Visualized key insights in Jupyter Notebook:
  - Total Sales by Region
  - Total Profit by Category
  - Average Profit Margin by Region

---

## ğŸ“Š Insights Summary
- The **West region** has the highest total sales.
- The **Technology category** generates the most profit.
- The **Central region** shows the highest average profit margin.

---

## ğŸ› ï¸ Tech Stack
- Python (Pandas, Matplotlib)
- Jupyter Notebook
- Virtual Environment (venv)
- Git + GitHub

---

## ğŸ§  Learning Outcomes
- Built an end-to-end ETL pipeline.
- Structured and automated data cleaning.
- Created simple but meaningful visualizations.
- Gained exposure to both Data Engineering and Data Analysis.

---

## ğŸ“ Folder Structure
data_engineering_project/
â”œâ”€â”€ data/
â”œâ”€â”€ scripts/
â”œâ”€â”€ notebooks/
â””â”€â”€ README.md


---

## ğŸŒŸ Next Steps
- Automate ETL with **Airflow** (intermediate level).
- Move processed data into a **SQL or cloud warehouse**.
- Visualize results in **Power BI or Tableau**.
